# @package _global_

defaults:
  - _self_
  - hydra: default

  - datamodule: from_local_files
  - model: default

  - learning_rate_scheduler@model.lr_scheduler_partial_fn: full

  - trainer: default
  - hardware: default

  - debug: null
  - experiment: null

task_name: "train"
output_dir: ${hydra:runtime.output_dir}

resume_from_checkpoint: null

seed: 1000

model:
  optimizer_partial_fn:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-4
    weight_decay: 0
