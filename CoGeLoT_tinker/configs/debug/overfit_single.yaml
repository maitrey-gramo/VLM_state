# @package _global_
defaults:
  - default.yaml
  - override /learning_rate_scheduler@model.lr_scheduler_partial_fn: constant
  - override /trainer/logger: none
  - override /trainer/callbacks:
      - model_summary
      - rich_progress
      - early_stopping_train_acc
  - _self_

task_name: "overfit-single-example"

datamodule:
  # Only show the rotate task
  task_index_seen: 2
  # Only show 1 example
  max_num_instances_seen: 1
  dataset_start_index: 0

model:
  optimizer_partial_fn:
    # Make the LRs bigger so it drops faster
    lr: 0.0001

trainer:
  log_every_n_steps: 1
  max_epochs: 500
  limit_train_batches: 1
  limit_val_batches: 0
  num_sanity_val_steps: 0

  enable_checkpointing: false
