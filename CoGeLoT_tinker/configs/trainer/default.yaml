defaults:
  - callbacks: default
  # - logger:
  #     - wandb_training
  #     - hf_model
  - _self_

_target_: pytorch_lightning.trainer.Trainer

default_root_dir: ${output_dir}

gradient_clip_val: 1.0

min_epochs: 1 # prevents early stopping

# https://github.com/vimalabs/VIMA/issues/16#issuecomment-1622973970
# max_epochs: 10
# Although they ran for 10 epochs, experiments have shown that nothing interesting happens after 6
# epochs, because the LR rate has plummeted to 1e-7 by then.
max_epochs: 10

accelerator: cpu
devices: 1

# mixed precision for extra speed-up
# precision: 16

# perform a validation loop every N training epochs
check_val_every_n_epoch: 1

# set True to to ensure deterministic results
# makes training slower but gives more reproducibility than just setting seeds
deterministic: False
