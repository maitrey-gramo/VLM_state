from typing import Literal, TypedDict

import cv2
import numpy as np
from einops import rearrange

from vima.utils import (
    DataDict,
    any_slice,
    any_stack,
    any_to_datadict,
    any_transpose_first_two_axes,
    get_batch_size,
)


class BrokeBboxException(ValueError):
    """Something is wrong with the bounding box."""


class ObsDict(TypedDict):
    """Dictionary of observations that they expect."""

    ee: np.ndarray
    rgb: dict[Literal["top", "front"], np.ndarray]
    segm: dict[Literal["top", "front"], np.ndarray]


def prepare_obs(
    *,
    obs: ObsDict,
    object_ids: list[int],
) -> DataDict:
    """Prepare observations for the model.

    Taken from `scripts/example.py:prepare_obs`. This might need making more efficient.
    """
    rgb_dict = obs.pop("rgb")
    segm_dict = obs.pop("segm")
    views = sorted(rgb_dict.keys())
    objects = object_ids

    L_obs = get_batch_size(obs)

    obs_list = {
        "ee": obs["ee"],
        "objects": {
            "cropped_img": {view: [] for view in views},
            "bbox": {view: [] for view in views},
            "mask": {view: [] for view in views},
        },
    }

    for obs_len in range(L_obs):
        rgb_dict_this_step = any_slice(rgb_dict, np.s_[obs_len])
        segm_dict_this_step = any_slice(segm_dict, np.s_[obs_len])
        for view in views:
            rgb_this_view = rgb_dict_this_step[view]
            segm_this_view = segm_dict_this_step[view]
            bboxes = []
            cropped_imgs = []
            n_pad = 0
            for obj_id in objects:
                ys, xs = np.nonzero(segm_this_view == obj_id)
                if len(xs) < 2 or len(ys) < 2:
                    n_pad += 1
                    continue
                xmin, xmax = np.min(xs), np.max(xs)
                ymin, ymax = np.min(ys), np.max(ys)
                x_center, y_center = (xmin + xmax) / 2, (ymin + ymax) / 2
                h, w = ymax - ymin, xmax - xmin
                bboxes.append([int(x_center), int(y_center), int(h), int(w)])
                cropped_img = rgb_this_view[:, ymin : ymax + 1, xmin : xmax + 1]
                if cropped_img.shape[1] != cropped_img.shape[2]:
                    diff = abs(cropped_img.shape[1] - cropped_img.shape[2])
                    pad_before, pad_after = int(diff / 2), diff - int(diff / 2)
                    if cropped_img.shape[1] > cropped_img.shape[2]:
                        pad_width = ((0, 0), (0, 0), (pad_before, pad_after))
                    else:
                        pad_width = ((0, 0), (pad_before, pad_after), (0, 0))
                    cropped_img = np.pad(
                        cropped_img, pad_width, mode="constant", constant_values=0
                    )
                    assert cropped_img.shape[1] == cropped_img.shape[2], "INTERNAL"
                cropped_img = rearrange(cropped_img, "c h w -> h w c")
                cropped_img = np.asarray(cropped_img).astype(np.float32)
                cropped_img = cv2.resize(
                    cropped_img,
                    (32, 32),
                    interpolation=cv2.INTER_AREA,
                )
                cropped_img = rearrange(cropped_img, "h w c -> c h w")
                cropped_imgs.append(cropped_img)
            bboxes = np.asarray(bboxes)
            cropped_imgs = np.asarray(cropped_imgs)
            mask = np.ones(len(bboxes), dtype=bool)
            if n_pad > 0:
                if bboxes.ndim == 1:
                    bboxes = np.expand_dims(bboxes, axis=0)
                try:
                    bboxes = np.concatenate(
                        [bboxes, np.zeros((n_pad, 4), dtype=bboxes.dtype)], axis=0
                    )
                except ValueError as err:
                    raise BrokeBboxException(f"bboxes: {bboxes}, n_pad: {n_pad}") from err

                cropped_imgs = np.concatenate(
                    [
                        cropped_imgs,
                        np.zeros(
                            (n_pad, 3, 32, 32),
                            dtype=cropped_imgs.dtype,
                        ),
                    ],
                    axis=0,
                )
                mask = np.concatenate([mask, np.zeros(n_pad, dtype=bool)], axis=0)
            obs_list["objects"]["bbox"][view].append(bboxes)
            obs_list["objects"]["cropped_img"][view].append(cropped_imgs)
            obs_list["objects"]["mask"][view].append(mask)
    for view in views:
        obs_list["objects"]["bbox"][view] = np.stack(obs_list["objects"]["bbox"][view], axis=0)
        obs_list["objects"]["cropped_img"][view] = np.stack(
            obs_list["objects"]["cropped_img"][view], axis=0
        )
        obs_list["objects"]["mask"][view] = np.stack(obs_list["objects"]["mask"][view], axis=0)

    obs = any_to_datadict(any_stack([obs_list], dim=0))
    obs = obs.to_torch_tensor()
    obs = any_transpose_first_two_axes(obs)
    return obs
